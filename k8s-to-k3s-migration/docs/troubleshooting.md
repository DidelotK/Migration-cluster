# üîß Guide de D√©pannage

Ce guide vous aide √† r√©soudre les probl√®mes courants lors de la migration K8s ‚Üí K3s.

## üö® Probl√®mes d'Infrastructure

### VM Scaleway ne d√©marre pas

**Sympt√¥mes :**
- Terraform timeout lors de la cr√©ation
- VM en √©tat "stopped" ou "starting"

**Diagnostic :**
```bash
# V√©rifier le statut via Terraform
cd infrastructure/terraform/environments/dev
terraform show

# V√©rifier via console Scaleway
# Ou via CLI Scaleway
scw instance server list
```

**Solutions :**
1. **Quota insuffisant** :
   ```bash
   # V√©rifier les quotas Scaleway
   scw account quota list
   ```

2. **Type d'instance non disponible** :
   ```bash
   # Changer le type dans terraform.tfvars
   vm_type = "DEV1-S"  # Au lieu de GP1-XS
   ```

3. **Zone non disponible** :
   ```bash
   # Changer la zone
   vm_zone = "fr-par-2"  # Au lieu de fr-par-1
   ```

### SSH ne fonctionne pas

**Sympt√¥mes :**
- `Permission denied (publickey)`
- `Connection timeout`

**Diagnostic :**
```bash
# Tester la connexion
ssh -i ssh-keys/vm-key ubuntu@[VM_IP] -v

# V√©rifier les permissions
ls -la ssh-keys/
```

**Solutions :**
```bash
# Corriger les permissions
chmod 600 ssh-keys/vm-key
chmod 644 ssh-keys/vm-key.pub

# Recr√©er la cl√© si n√©cessaire
cd infrastructure/terraform/environments/dev
terraform taint module.vm.tls_private_key.vm_key
terraform apply
```

## üê≥ Probl√®mes K3s

### K3s ne d√©marre pas

**Sympt√¥mes :**
- Service k3s failed
- Pods en CrashLoopBackOff

**Diagnostic :**
```bash
# Connexion √† la VM
ssh -i ssh-keys/vm-key ubuntu@[VM_IP]

# V√©rifier le service K3s
sudo systemctl status k3s
sudo journalctl -u k3s -f

# V√©rifier les ressources
df -h
free -h
```

**Solutions :**
1. **Espace disque insuffisant** :
   ```bash
   # Nettoyer les images Docker
   sudo k3s crictl images prune
   
   # Augmenter la taille du disque dans Terraform
   root_volume_size = 120  # Au lieu de 80
   ```

2. **M√©moire insuffisante** :
   ```bash
   # V√©rifier l'utilisation m√©moire
   sudo k3s kubectl top nodes
   
   # Upgrade le type de VM
   vm_type = "GP1-S"  # Plus de RAM
   ```

### Pods en ImagePullBackOff

**Sympt√¥mes :**
- `ErrImagePull`
- `ImagePullBackOff`

**Diagnostic :**
```bash
# V√©rifier les √©v√©nements
kubectl --kubeconfig=kubeconfig-target.yaml get events --sort-by='.lastTimestamp'

# V√©rifier les secrets Docker
kubectl --kubeconfig=kubeconfig-target.yaml get secrets -A | grep registry
```

**Solutions :**
```bash
# Recr√©er les secrets Docker Registry
./k8s-to-k3s-migration/scripts/setup-secrets.sh docker

# V√©rifier les credentials Scaleway
echo $SCW_SECRET_KEY

# Tester manuellement
docker login rg.fr-par.scw.cloud -u nologin -p $SCW_SECRET_KEY
```

### Ingress ne fonctionne pas

**Sympt√¥mes :**
- Sites inaccessibles
- `404 Not Found` ou `Connection refused`

**Diagnostic :**
```bash
# V√©rifier Ingress Nginx
kubectl --kubeconfig=kubeconfig-target.yaml get pods -n ingress-nginx

# V√©rifier les logs
kubectl --kubeconfig=kubeconfig-target.yaml logs -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx

# V√©rifier les Ingress
kubectl --kubeconfig=kubeconfig-target.yaml get ingress -A
```

**Solutions :**
1. **Ports non ouverts** :
   ```bash
   # V√©rifier les ports sur la VM
   ssh -i ssh-keys/vm-key ubuntu@[VM_IP]
   sudo netstat -tlnp | grep :80
   sudo netstat -tlnp | grep :443
   ```

2. **Ingress Controller mal configur√©** :
   ```bash
   # Red√©ployer Ingress Nginx
   kubectl --kubeconfig=kubeconfig-target.yaml delete namespace ingress-nginx
   
   # Relancer Ansible
   cd infrastructure/terraform/environments/dev
   terraform apply -replace="null_resource.ansible_provisioner"
   ```

## üîê Probl√®mes SSL/TLS

### Certificats Let's Encrypt √©chouent

**Sympt√¥mes :**
- `NET::ERR_CERT_AUTHORITY_INVALID`
- Certificate requests failed

**Diagnostic :**
```bash
# V√©rifier Cert Manager
kubectl --kubeconfig=kubeconfig-target.yaml get pods -n cert-manager

# V√©rifier les certificats
kubectl --kubeconfig=kubeconfig-target.yaml get certificates -A

# V√©rifier les challenges
kubectl --kubeconfig=kubeconfig-target.yaml get challenges -A
```

**Solutions :**
1. **Firewall bloque HTTP-01** :
   ```bash
   # V√©rifier les security groups Scaleway
   scw instance security-group list
   
   # Ajouter r√®gles HTTP/HTTPS si manquantes
   # Ports 80 et 443 doivent √™tre ouverts
   ```

2. **DNS ne pointe pas vers le bon IP** :
   ```bash
   # V√©rifier la r√©solution DNS
   dig vault1.keltio.fr +short
   
   # Doit retourner l'IP de votre VM K3s
   ```

3. **Rate limiting Let's Encrypt** :
   ```bash
   # Utiliser staging temporairement
   kubectl --kubeconfig=kubeconfig-target.yaml patch clusterissuer letsencrypt-prod -p '{"spec":{"acme":{"server":"https://acme-staging-v02.api.letsencrypt.org/directory"}}}'
   ```

## üåê Probl√®mes DNS

### External DNS ne cr√©e pas les enregistrements

**Sympt√¥mes :**
- DNS records not created
- External DNS logs errors

**Diagnostic :**
```bash
# V√©rifier External DNS
kubectl --kubeconfig=kubeconfig-target.yaml logs -n kube-system -l app.kubernetes.io/name=external-dns

# V√©rifier les secrets Scaleway
kubectl --kubeconfig=kubeconfig-target.yaml get secret external-dns-scaleway -n kube-system -o yaml
```

**Solutions :**
1. **Credentials Scaleway incorrects** :
   ```bash
   # V√©rifier les variables
   echo $SCW_ACCESS_KEY
   echo $SCW_SECRET_KEY
   
   # Recr√©er le secret
   ./k8s-to-k3s-migration/scripts/setup-secrets.sh external-dns
   ```

2. **Permissions insuffisantes** :
   ```bash
   # V√©rifier les permissions du token Scaleway
   # Le token doit avoir acc√®s DNS
   ```

### Cloudflare Proxy activ√©

**Sympt√¥mes :**
- SSL errors avec Cloudflare
- Redirections infinies

**Solutions :**
```bash
# D√©sactiver le proxy Cloudflare (nuage orange ‚Üí gris)
# Via interface web ou API :

curl -X PATCH "https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records/$RECORD_ID" \
  -H "X-Auth-Email: $EMAIL" \
  -H "X-Auth-Key: $API_KEY" \
  -H "Content-Type: application/json" \
  --data '{"proxied":false}'
```

## üíæ Probl√®mes de Migration de Donn√©es

### Vaultwarden - Base de donn√©es corrompue

**Sympt√¥mes :**
- `database is locked`
- `readonly database`

**Diagnostic :**
```bash
# V√©rifier les logs Vaultwarden
kubectl --kubeconfig=kubeconfig-target.yaml logs -n vaultwarden vaultwarden-0

# V√©rifier les permissions fichiers
kubectl --kubeconfig=kubeconfig-target.yaml exec vaultwarden-0 -n vaultwarden -- ls -la /data/
```

**Solutions :**
```bash
# Corriger les permissions
kubectl --kubeconfig=kubeconfig-target.yaml scale statefulset vaultwarden --replicas=0 -n vaultwarden

# Pod temporaire pour corriger
kubectl --kubeconfig=kubeconfig-target.yaml apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: fix-vaultwarden
  namespace: vaultwarden
spec:
  containers:
  - name: fix
    image: busybox
    command: ['sleep', '3600']
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: vaultwarden-data-vaultwarden-0
EOF

# Corriger permissions et ownership
kubectl --kubeconfig=kubeconfig-target.yaml exec fix-vaultwarden -n vaultwarden -- chown -R 1000:1000 /data
kubectl --kubeconfig=kubeconfig-target.yaml exec fix-vaultwarden -n vaultwarden -- chmod 755 /data
kubectl --kubeconfig=kubeconfig-target.yaml exec fix-vaultwarden -n vaultwarden -- chmod 644 /data/db.sqlite3*

# Nettoyer et red√©marrer
kubectl --kubeconfig=kubeconfig-target.yaml delete pod fix-vaultwarden -n vaultwarden
kubectl --kubeconfig=kubeconfig-target.yaml scale statefulset vaultwarden --replicas=1 -n vaultwarden
```

### Monitoring - Dashboards perdus

**Diagnostic :**
```bash
# V√©rifier si les donn√©es Grafana sont pr√©sentes
kubectl --kubeconfig=kubeconfig-target.yaml exec -n monitoring deployment/kube-prometheus-stack-grafana -- ls -la /var/lib/grafana/
```

**Solutions :**
```bash
# R√©importer les dashboards depuis l'export
kubectl --kubeconfig=kubeconfig-target.yaml scale deployment kube-prometheus-stack-grafana --replicas=0 -n monitoring

# R√©importer les donn√©es
# (Utilisez le script de migration de donn√©es)

kubectl --kubeconfig=kubeconfig-target.yaml scale deployment kube-prometheus-stack-grafana --replicas=1 -n monitoring
```

## üîç Outils de Diagnostic

### Scripts d'aide

```bash
# Script de diagnostic global
cat > diagnostic.sh << 'EOF'
#!/bin/bash
echo "=== Diagnostic K3s Migration ==="
echo "Cluster status:"
kubectl --kubeconfig=kubeconfig-target.yaml get nodes -o wide

echo -e "\nPods status:"
kubectl --kubeconfig=kubeconfig-target.yaml get pods -A | grep -v Running

echo -e "\nIngress status:"
kubectl --kubeconfig=kubeconfig-target.yaml get ingress -A

echo -e "\nCertificates status:"
kubectl --kubeconfig=kubeconfig-target.yaml get certificates -A

echo -e "\nSecrets status:"
kubectl --kubeconfig=kubeconfig-target.yaml get secrets -A | grep -E "scaleway|external-dns"

echo -e "\nDisk usage:"
kubectl --kubeconfig=kubeconfig-target.yaml exec -n kube-system daemonset/local-path-provisioner -- df -h

echo -e "\nRecent events:"
kubectl --kubeconfig=kubeconfig-target.yaml get events --sort-by='.lastTimestamp' | tail -10
EOF

chmod +x diagnostic.sh
./diagnostic.sh
```

### Validation r√©seau

```bash
# Test connectivit√© depuis les pods
kubectl --kubeconfig=kubeconfig-target.yaml run test-pod --rm -it --image=busybox -- sh

# Dans le pod :
nslookup kubernetes.default.svc.cluster.local
wget -qO- http://google.com
```

## üìû Escalade

Si les solutions ci-dessus ne r√©solvent pas votre probl√®me :

1. **Collecter les logs** complets
2. **Documenter** les √©tapes de reproduction
3. **V√©rifier** les issues GitHub existantes
4. **Cr√©er** une nouvelle issue avec :
   - Description du probl√®me
   - Logs pertinents
   - Configuration syst√®me
   - √âtapes pour reproduire

## üÜò Recovery

### Sauvegarde d'urgence

```bash
# Sauvegarder etcd K3s
ssh -i ssh-keys/vm-key ubuntu@[VM_IP]
sudo k3s etcd-snapshot save emergency-backup

# Exporter tous les manifests
kubectl --kubeconfig=kubeconfig-target.yaml get all -A -o yaml > emergency-export.yaml
```

### Restauration

```bash
# Restaurer etcd
sudo k3s server --cluster-reset --cluster-reset-restore-path=emergency-backup

# R√©appliquer les manifests
kubectl --kubeconfig=kubeconfig-target.yaml apply -f emergency-export.yaml
```

---

**üí° Tip :** Gardez toujours une sauvegarde de votre cluster source jusqu'√† validation compl√®te de la migration !
